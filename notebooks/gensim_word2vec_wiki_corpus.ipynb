{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "84163f3ca19c0b7c9fda47121b3bc4cadfaf1fcc"
   },
   "source": [
    "# Gensim word2vec for Wikipedia corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cc7b3e6ca62670ff13626705402f626778487204"
   },
   "outputs": [],
   "source": [
    "import re  # For preprocessing\n",
    "import pandas as pd  # For data handling\n",
    "from time import time  # To time our operations\n",
    "from collections import defaultdict  # For word frequency\n",
    "\n",
    "import spacy  # For preprocessing\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "cwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6453b9c3f797e51923e030090ead659253f4e459",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus = open(\"../output/wikipedia2008_fi_lemmatized.txt\").read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7f07dca2a2656dcd9e0c315afa36af32a992eef7"
   },
   "source": [
    "## Alternative cleaning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Wiki data is already lemmatized, I will split the data at periods to get sentences and then split sentences at whitespace\n",
    "\n",
    "Resulting data: list of lists "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# speeches is a list of list of sentences\n",
    "sentences = re.split(r\"[.!?]\", corpus) \n",
    "\n",
    "# sentences as list of words\n",
    "sents = [sent.strip().split() for sent in sentences if sent != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "31b4a744059df490ddb47ab6cdec008dc929ede3"
   },
   "source": [
    "## Bigrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "af6d420284a0ff7a7407d4c526754ffe850d6170"
   },
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bb7766b322cbc1d3381912b890585eb249ac5304"
   },
   "source": [
    "Creates the relevant phrases from the list of sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8befad8c76c54bd2b831b0942a2f626f7d8a6dac"
   },
   "outputs": [],
   "source": [
    "phrases = Phrases(sents, min_count=30, progress_per=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "45bae4a953f2ad8951e4efb234e1e357857a33b3"
   },
   "source": [
    "The goal of Phraser() is to cut down memory consumption of Phrases(), by discarding model state not strictly needed for the bigram detection task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "b8ae81ba230013aefe7c584338de7376fedf6294"
   },
   "outputs": [],
   "source": [
    "bigram = Phraser(phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4a58380f19d159688aeee665d1afb96289fdd4b8"
   },
   "source": [
    "Transform the corpus based on the bigrams detected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8051b56890c147119db3df529d3cfd3cf675fdca"
   },
   "outputs": [],
   "source": [
    "sentences = bigram[sents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a4f81e8bb2c09a67b00cd24db28353eca8ae188c"
   },
   "source": [
    "## Most Frequent Words:\n",
    "Mainly a sanity check of the effectiveness of the lemmatization, removal of stopwords, and addition of bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "eeb8afe1cfcb7ba65bd14d657455600acacf39ba"
   },
   "outputs": [],
   "source": [
    "word_freq = defaultdict(int)\n",
    "for sent in sentences:\n",
    "    for i in sent:\n",
    "        word_freq[i] += 1\n",
    "len(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5b010149150b2b2eaf332d79bcde0649b8a3c2b5"
   },
   "outputs": [],
   "source": [
    "sorted(word_freq, key=word_freq.get, reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "500ab7b5c84dc006d7945f339c40725a82856fdf"
   },
   "source": [
    "# Training the model\n",
    "## Gensim Word2Vec Implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3269be205cadbad499aa87890893d92da6adc796"
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7c524bc49c41a6c37f9e754a38797c9501202090"
   },
   "source": [
    "## Why I seperate the training of the model in 3 steps:\n",
    "I prefer to separate the training in 3 distinctive steps for clarity and monitoring.\n",
    "1. `Word2Vec()`: \n",
    ">In this first step, I set up the parameters of the model one-by-one. <br>I do not supply the parameter `sentences`, and therefore leave the model uninitialized, purposefully.\n",
    "2. `.build_vocab()`: \n",
    ">Here it builds the vocabulary from a sequence of sentences and thus initialized the model. <br>With the loggings, I can follow the progress and even more important, the effect of `min_count` and `sample` on the word corpus. I noticed that these two parameters, and in particular `sample`, have a great influence over the performance of a model. Displaying both allows for a more accurate and an easier management of their influence.\n",
    "3. `.train()`:\n",
    ">Finally, trains the model.<br>\n",
    "The loggings here are mainly useful for monitoring, making sure that no threads are executed instantaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "03488d9b68963579c96094aca88a302c9f2753a7"
   },
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count() # Count the number of cores in a computer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "89c305fcd163488441ac2ac6133678bd973b4419"
   },
   "source": [
    "## The parameters:\n",
    "\n",
    "* `min_count` <font color='purple'>=</font> <font color='green'>int</font> - Ignores all words with total absolute frequency lower than this - (2, 100)\n",
    "\n",
    "\n",
    "* `window` <font color='purple'>=</font> <font color='green'>int</font> - The maximum distance between the current and predicted word within a sentence. E.g. `window` words on the left and `window` words on the left of our target - (2, 10)\n",
    "\n",
    "\n",
    "* `size` <font color='purple'>=</font> <font color='green'>int</font> - Dimensionality of the feature vectors. - (50, 300)\n",
    "\n",
    "\n",
    "* `sample` <font color='purple'>=</font> <font color='green'>float</font> - The threshold for configuring which higher-frequency words are randomly downsampled. Highly influencial.  - (0, 1e-5)\n",
    "\n",
    "\n",
    "* `alpha` <font color='purple'>=</font> <font color='green'>float</font> - The initial learning rate - (0.01, 0.05)\n",
    "\n",
    "\n",
    "* `min_alpha` <font color='purple'>=</font> <font color='green'>float</font> - Learning rate will linearly drop to `min_alpha` as training progresses. To set it: alpha - (min_alpha * epochs) ~ 0.00\n",
    "\n",
    "\n",
    "* `negative` <font color='purple'>=</font> <font color='green'>int</font> - If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\" should be drown. If set to 0, no negative sampling is used. - (5, 20)\n",
    "\n",
    "\n",
    "* `workers` <font color='purple'>=</font> <font color='green'>int</font> - Use these many worker threads to train the model (=faster training with multicore machines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ad619db82c219d6cb81fad516563feb0c4d474cd"
   },
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(min_count=20,\n",
    "                     window=2,\n",
    "                     size=300,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=cores-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d7e9f1bd338f9e15647b5209ffd8fbb131cd7ee5"
   },
   "source": [
    "## Building the Vocabulary Table:\n",
    "Word2Vec requires us to build the vocabulary table (simply digesting all the words and filtering out the unique words, and doing some basic counts on them):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "66358ad743e05e17dfbed3899af9c41056143daa"
   },
   "outputs": [],
   "source": [
    "t = time()\n",
    "\n",
    "w2v_model.build_vocab(sentences, progress_per=10000)\n",
    "\n",
    "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "63260d82061abb47db7f2f8b23e07ec629adf5a9"
   },
   "source": [
    "## Training of the model:\n",
    "_Parameters of the training:_\n",
    "* `total_examples` <font color='purple'>=</font> <font color='green'>int</font> - Count of sentences;\n",
    "* `epochs` <font color='purple'>=</font> <font color='green'>int</font> - Number of iterations (epochs) over the corpus - [10, 20, 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "07a2a047e701e512fd758edff186daadaeea6461"
   },
   "outputs": [],
   "source": [
    "t = time()\n",
    "\n",
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.save(\"w2v_model.model.alt_preprocessing.wiki\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "34dd51c7f2f39d016b982ef81e4df576f6b31bcb"
   },
   "outputs": [],
   "source": [
    "# Make model more memory-efficient\n",
    "w2v_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a420d5a98eb860cff1f4bbac8cbe2054459b6200"
   },
   "source": [
    "# Exploring the model\n",
    "## Most similar to:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "339207a733a1ac42fe60e32a29f9e5d5ca0a9275"
   },
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"keskusta\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3b6686e6fa956a98450259b063b4cf51019a6d0b"
   },
   "source": [
    "_A small precision here:_<br>\n",
    "The dataset is the Simpsons' lines of dialogue; therefore, when we look at the most similar words from \"homer\" we do **not** necessary get his family members, personality traits, or even his most quotable words. No, we get what other characters (as Homer does not often refers to himself at the 3rd person) said along with \"homer\", such as how he feels or looks (\"depressed\"), where he is (\"hammock\"), or with whom (\"marge\").\n",
    "\n",
    "Let's see what the bigram \"homer_simpson\" gives us by comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "22595f98c675a9697243b7e826b2840e5fc3e5f5"
   },
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"juha\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ac9ba47738e596dce6552099e76f303f28577943"
   },
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"sipil\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"orpo\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d8b5937dfd7584f168a33060c435036cad5b390b"
   },
   "source": [
    "## Similarities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "367755f5c9e00de4bc5056c978f5b50a38c1368b"
   },
   "outputs": [],
   "source": [
    "w2v_model.wv.similarity(\"keskusta\", 'kokoomus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "349828078b5a438d93e5494478e88095913dc58e"
   },
   "outputs": [],
   "source": [
    "w2v_model.wv.similarity('juha_sipil', 'orpo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "08a999d758ac687d626b631a8ce393eaa26f41e7"
   },
   "source": [
    "## Odd-One-Out:\n",
    "\n",
    "Here, we ask our model to give us the word that does not belong to the list!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d982e44d9c212b5ee09bcaebd050a725ab5e508e"
   },
   "outputs": [],
   "source": [
    "w2v_model.wv.doesnt_match(['keskusta', 'juha_sipil', 'orpo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.doesnt_match(['kokoomus', 'juha_sipil', 'orpo'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "df95cdff693e843ab4b4c174fea24029447573cd"
   },
   "source": [
    "## Analogy difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "812961e79dde9f2032f708755ca287c0aef838d0"
   },
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"orpo\"], negative=[\"keskusta\",\"sipil\"], topn=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "773c0acc8750ba8e728ff261f2e9ec39694c245c"
   },
   "source": [
    "### t-SNE visualizations:\n",
    "t-SNE is a non-linear dimensionality reduction algorithm that attempts to represent high-dimensional data and the underlying relationships between vectors in a lower-dimensional space.<br>\n",
    "Here is a good tutorial on it: https://medium.com/@luckylwk/visualising-high-dimensional-datasets-using-pca-and-t-sne-in-python-8ef87e7915b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "27ec46110042fc28da900b1b344ae4e0692d5dc2"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    " \n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "22693eaa25253b38cee3c5cd5db6b6fdddb575a4"
   },
   "source": [
    "Our goal in this section is to plot our 300 dimensions vectors into 2 dimensional graphs, and see if we can spot interesting patterns.<br>\n",
    "For that we are going to use t-SNE implementation from scikit-learn.\n",
    "\n",
    "To make the visualizations more relevant, we will look at the relationships between a query word (in <font color='red'>**red**</font>), its most similar words in the model (in <font color=\"blue\">**blue**</font>), and other words from the vocabulary (in <font color='green'>**green**</font>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "489a7d160dcd92da0ce42a3b5b461368c9ffe5f1"
   },
   "outputs": [],
   "source": [
    "def tsnescatterplot(model, word, list_names):\n",
    "    \"\"\" Plot in seaborn the results from the t-SNE dimensionality reduction algorithm of the vectors of a query word,\n",
    "    its list of most similar words, and a list of words.\n",
    "    \"\"\"\n",
    "    arrays = np.empty((0, 300), dtype='f')\n",
    "    word_labels = [word]\n",
    "    color_list  = ['red']\n",
    "\n",
    "    # adds the vector of the query word\n",
    "    arrays = np.append(arrays, model.wv.__getitem__([word]), axis=0)\n",
    "    \n",
    "    # gets list of most similar words\n",
    "    close_words = model.wv.most_similar([word])\n",
    "    \n",
    "    # adds the vector for each of the closest words to the array\n",
    "    for wrd_score in close_words:\n",
    "        wrd_vector = model.wv.__getitem__([wrd_score[0]])\n",
    "        word_labels.append(wrd_score[0])\n",
    "        color_list.append('blue')\n",
    "        arrays = np.append(arrays, wrd_vector, axis=0)\n",
    "    \n",
    "    # adds the vector for each of the words from list_names to the array\n",
    "    for wrd in list_names:\n",
    "        wrd_vector = model.wv.__getitem__([wrd])\n",
    "        word_labels.append(wrd)\n",
    "        color_list.append('green')\n",
    "        arrays = np.append(arrays, wrd_vector, axis=0)\n",
    "        \n",
    "    # Reduces the dimensionality from 300 to 50 dimensions with PCA\n",
    "    reduc = PCA(n_components=21).fit_transform(arrays)\n",
    "    \n",
    "    # Finds t-SNE coordinates for 2 dimensions\n",
    "    np.set_printoptions(suppress=True)\n",
    "    \n",
    "    Y = TSNE(n_components=2, random_state=0, perplexity=15).fit_transform(reduc)\n",
    "    \n",
    "    # Sets everything up to plot\n",
    "    df = pd.DataFrame({'x': [x for x in Y[:, 0]],\n",
    "                       'y': [y for y in Y[:, 1]],\n",
    "                       'words': word_labels,\n",
    "                       'color': color_list})\n",
    "    \n",
    "    fig, _ = plt.subplots()\n",
    "    fig.set_size_inches(9, 9)\n",
    "    \n",
    "    # Basic plot\n",
    "    p1 = sns.regplot(data=df,\n",
    "                     x=\"x\",\n",
    "                     y=\"y\",\n",
    "                     fit_reg=False,\n",
    "                     marker=\"o\",\n",
    "                     scatter_kws={'s': 40,\n",
    "                                  'facecolors': df['color']\n",
    "                                 }\n",
    "                    )\n",
    "    \n",
    "    # Adds annotations one by one with a loop\n",
    "    for line in range(0, df.shape[0]):\n",
    "         p1.text(df[\"x\"][line],\n",
    "                 df['y'][line],\n",
    "                 '  ' + df[\"words\"][line].title(),\n",
    "                 horizontalalignment='left',\n",
    "                 verticalalignment='bottom', size='medium',\n",
    "                 color=df['color'][line],\n",
    "                 weight='normal'\n",
    "                ).set_size(15)\n",
    "\n",
    "    \n",
    "    plt.xlim(Y[:, 0].min()-50, Y[:, 0].max()+50)\n",
    "    plt.ylim(Y[:, 1].min()-50, Y[:, 1].max()+50)\n",
    "            \n",
    "    plt.title('t-SNE visualization for {}'.format(word.title()))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c73fc2faaf0baecc84f02a97b50cb9ccefa48686"
   },
   "source": [
    "## 10 Most similar words vs. 10 Most dissimilar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "10c77b072f7c281f2be919341be116565c20d8a8"
   },
   "outputs": [],
   "source": [
    "tsnescatterplot(w2v_model, 'keskusta', [i[0] for i in w2v_model.wv.most_similar(negative=[\"orpo\"])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "87315bfbaceb3733bd7af035db6c59cfc4b1ba7f"
   },
   "source": [
    "## 10 Most similar words vs. 11th to 20th Most similar words:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e6f0bc598922f4f2cd17d2511560242a3c35fdd9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tsnescatterplot(w2v_model, \"sipil\", [t[0] for t in w2v_model.wv.most_similar(positive=[\"keskusta\"], topn=20)][10:])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
